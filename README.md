# metaRAG

![FastAPI](https://img.shields.io/badge/FastAPI-009688?style=for-the-badge&logo=fastapi&logoColor=white)
![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)
![ChromaDB](https://img.shields.io/badge/ChromaDB-000000?style=for-the-badge&logo=chroma&logoColor=white)
![Ollama](https://img.shields.io/badge/Ollama-17022A?style=for-the-badge&logo=ollama&logoColor=white)
![Pydantic](https://img.shields.io/badge/Pydantic-E92063?style=for-the-badge&logo=pydantic&logoColor=white)

## Introduction

This project implements a Retrieval Augmented Generation (RAG) system exposed as RESTful API using FastAPI. The core purpose is to enable users to securely upload their private text documents or files (PDF/TXT), store them in a searchable format, and then query them to get contextually relevant answers generated by a Large Language Model (LLM).

The fundamental problem addressed is the need for personalized AI interactions based on specific, often private, data. By integrating vector databases with LLMs, RAG systems overcome the limitations of base LLMs (which only know what they were trained on) and knowledge cutoff dates, allowing them to provide accurate, up-to-date, and source-attributable answers grounded in specific documents. This implementation further enhances this by incorporating **metadata filtering**, ensuring queries are scoped to specific users or categories of documents.

**metaRAG** is built on:

1.  **FastAPI:** Serves as the high-performance web framework for building the API. It provides the structure for defining endpoints (`/upload/text`, `/upload/file`, `/ask`, `/health`), handling HTTP requests, and managing the response cycle. Its asynchronous capabilities (`async def`) allow it to handle multiple requests efficiently. FastAPI also automatically generates interactive API documentation (`Swagger UI` and `ReDoc`) based on the code structure and Pydantic models.
2.  **Pydantic:** Integrated seamlessly with FastAPI, Pydantic is used for defining clear data models (`BaseDocumentMetadata`, `TextUploadRequest`, `AskRequest`, `ApiResponse`, `AskResponseData`, `ContextItem`) for request and response bodies. This ensures automatic data validation, serialization, and helps FastAPI generate accurate OpenAPI documentation. It enforces that required fields, like `user_id` and `language` in metadata, are present and correctly typed.
3.  **ChromaDB:** This acts as the persistent vector store. It's responsible for storing the numerical representations (embeddings) of document chunks, along with their original text content and associated metadata. During a query, ChromaDB efficiently searches for embeddings similar to the query embedding and, crucially, applies the specified metadata filters (`user_metadata_filter`) to retrieve only relevant chunks from the user's permitted data subset.
4.  **Ollama:** This serves as the engine for running various Language Models. It provides APIs to generate document and query embeddings using dedicated embedding models (like `nomic-embed-text` or `granite-embedding`) and to generate text answers using larger LLMs (like `llama3` or `mistral-nemo`). The application interacts with Ollama via its Python client library. The configuration allows specifying different embedding and LLM models based on the document/query language.
5.  **Langchain Text Splitters:** A utility library used to break down large pieces of text (from documents or direct input) into smaller, fixed-size `chunks`. This is essential because embedding models and RAG systems perform better with smaller, self-contained pieces of information. The `RecursiveCharacterTextSplitter` is used here to intelligently split text while maintaining context where possible.
6.  **pypdf:** A library specifically used to extract text content from uploaded PDF files, enabling the system to process this common document format.
7.  **uvicorn:** ASGI server to run the FastAPI application.

**How `metaRAG` orchestrates the activities of the RAG:**

The Python code (`metaRAG.py`) ties these components together within the `metaRAG` class and the FastAPI application instance (`app`).

*   **Initialization:** On server startup, the FastAPI `lifespan` context manager initializes a single instance of the `metaRAG` class. This `metaRAG` instance connects to ChromaDB (creating or loading the collection) and verifies connectivity and model availability with Ollama.
*   **Ingestion Workflow (`/upload/text`, `/upload/file`):**
    *   FastAPI receives the request and uses Pydantic to validate the input (text/file content and metadata).
    *   The request handler passes the validated data to the `metaRAG` instance (`upload_text` or `upload_document` methods).
    *   Inside `metaRAG`:
        *   File uploads trigger text extraction using `pypdf`.
        *   Text content (either raw text or extracted from a file) is passed to `langchain_text_splitters` to create chunks.
        *   For each chunk, the `ollama` client is called to generate an embedding using the appropriate language-specific model (determined by the metadata's `language`).
        *   The chunk's text, its embedding, and the document's metadata (including the user-provided fields and system-added `source_filename`) are added to the `chromadb` collection using the `chromadb` client. Metadata values that are complex Python types (like dicts or lists) are automatically serialized to JSON strings before storage.
*   **Query Workflow (`/ask`):**
    *   FastAPI receives the query and uses Pydantic (`AskRequest`) to validate the input, including the query string and the mandatory `query_metadata_filter`.
    *   The request handler passes the validated data to the `metaRAG.ask` method.
    *   Inside `metaRAG.ask`:
        *   The `ollama` client generates an embedding for the user's query using the language-specific embedding model matching the filter's `language`.
        *   The `chromadb` collection is queried using the query embedding. The `query_metadata_filter` (which *must* include `user_id` and `language`, and can include other custom fields) is passed to ChromaDB's `where` clause to ensure only relevant documents are searched.
        *   ChromaDB returns the most similar document chunks matching the filter.
        *   The text content of the retrieved chunks is compiled into a single context string.
        *   The `ollama` client is called again, this time with the language-specific LLM, providing the compiled context and the original query. The LLM then generates a response based *only* the context provided.
        *   The generated answer and the details of the retrieved context items (including their deserialized metadata) are structured into the `AskResponseData` Pydantic model, wrapped in an `AskApiResponse`, and returned by the FastAPI endpoint.

In essence, FastAPI acts as the external interface and data guardian, Pydantic ensures data integrity, `metaRAG` is the internal brain coordinating the RAG process, relying on: Ollama for model inference, ChromaDB for vector search and storage, and the splitting/parsing libraries for document preparation. The `lifespan` ensures the core `metaRAG` component is ready when the API starts.

## Features

The metaRAG FastAPI Server offers the following key capabilities:

*   **Document and Text Ingestion:**
    *   Provides API endpoints (`/upload/text/` and `/upload/file/`) to get your data into the RAG system. You can directly submit plain text or upload common document formats like PDF and TXT: this is the essential first step for any RAG system – populating the knowledge base with the documents you want to query.
    *   The FastAPI endpoints receive the data. For files, the code temporarily saves the file and uses `pypdf` (for PDFs) or standard file reading (for TXT) to extract the raw text content. This text content is then passed down the ingestion pipeline. Pydantic models (`TextUploadRequest`, `BaseDocumentMetadata`) ensure the incoming data format is correct.
*   **Automatic Chunking:**
    *   Automatically breaks down long text content (from uploads) into smaller, more manageable segments called "chunks": LLM context windows are limited, and smaller chunks allow the vector database to find more precise relevant pieces of information during retrieval. Overlap between chunks helps ensure that context isn't lost at the boundaries of the split.
    *   The `metaRAG` class uses `langchain_text_splitters.RecursiveCharacterTextSplitter` configured with a specific `chunk_size` and `chunk_overlap`. After text extraction (for files) or receiving the raw text, the `split_text` method is called to generate the list of chunks.
*   **Rich Metadata Handling and Filtering:**
    *   Allows associating custom key-value metadata (e.g., `user_id`, `language`, `topic`, `project`, `date`, etc.) with each document during upload. This metadata is stored alongside the document chunks. Crucially, query operations can include metadata filters to retrieve context only from relevant documents. This is essential for multi-tenancy (isolating one user's data from another's), categorizing documents, and refining query results. Mandatory `user_id` and `language` fields ensure data segregation and enable language-specific model selection.
    *   The `BaseDocumentMetadata` Pydantic model allows for `extra="allow"`, meaning you can include any custom fields beyond the required `user_id` and `language`. During ingestion (`_process_and_store_chunks`), this metadata is stored with each chunk in ChromaDB. During querying (`_retrieve_context`), the `query_metadata_filter` (also based on `BaseDocumentMetadata`) is translated into ChromaDB's `where` filter condition, ensuring searches are limited to chunks matching *all* specified criteria. The `_serialize_metadata_values` method handles converting complex Python objects in metadata to ChromaDB-compatible types (JSON strings).
*   **Vector Embedding Generation:**
    *   Converts the text chunks (during ingestion) and the user's query (during retrieval) into high-dimensional numerical vectors (embeddings): embeddings capture the semantic meaning of the text. By converting text into vectors, the system can use mathematical operations (like calculating cosine similarity) to find text segments with similar meanings, regardless of exact keyword matches.
    *   The `metaRAG` class interacts with the `ollama.Client`. The `_get_ollama_embedding` method calls the Ollama service, specifying the appropriate *embedding model* based on the language associated with the text/query (determined by the `language` metadata and the `_get_models_for_language` helper).
*   **Persistent Vector Storage:**
    *   Stores the generated vector embeddings, the original text chunks, and their associated metadata in a specialized database designed for vector search. This provides efficient storage and retrieval of billions of vectors and persistence ensures the data is available even after the server restarts, building a durable knowledge base.
    *   The project uses `chromadb.PersistentClient` to implement it. The `metaRAG` instance connects to a persistent collection (defaulting to `./metaRAG_db/metaRAG_collection`). The `_process_and_store_chunks` method uses the `collection.add()` method to store the chunks, embeddings, and metadata.
*   **Intelligent Contextual Retrieval:**
    *   Given a user query and metadata filters, the system searches the vector store to find the most semantically relevant document chunks that also match the specified metadata: finding the *right* pieces of information is crucial for providing accurate answers. Combining vector similarity with metadata filtering ensures both semantic relevance *and* adherence to constraints like user access or document category.
    *   The `_retrieve_context` method takes the query embedding and the `query_metadata_filter_dict`. It calls `self.collection.query()`, passing the query embedding, the desired number of results (`n_results`), and constructing ChromaDB's `where` filter from the provided metadata dictionary. The returned `documents`, `metadatas`, and `distances` are then structured into `ContextItem` Pydantic models.
*   **Language Model (LLM) Grounding:**
    *   Uses an LLM to synthesize a coherent and relevant answer based *only* on the retrieved document chunks and the user's original query: the LLM acts as the reasoning and text generation engine. By grounding the LLM's response in the retrieved context, the system minimizes the risk of hallucination (making up facts) and provides answers that are directly supported by the user's uploaded data.
    *   The `ask` method takes the retrieved context items and the user query. It selects the appropriate *LLM model* using `_get_models_for_language` based on the filter's language. A prompt is constructed using a `prompt_template` that explicitly instructs the LLM to answer based *only* on the provided context. This prompt is sent to the Ollama service via `ollama_client.chat()`, and the LLM's response is extracted as the final answer.
*   **Multi-language Support:**
    *   The system can handle and process documents and queries in different languages for different users. Using language-specific embedding models generally improves retrieval accuracy, and using LLMs suitable for the target language enhances answer quality.
    *   The `DEFAULT_LANGUAGE_MODELS` dictionary maps language codes (e.g., "en", "it") to specific Ollama embedding and LLM models. The `language` field in the metadata is mandatory and is used by the `_get_models_for_language` helper to dynamically select the correct models for both embedding generation (ingestion and query) and LLM inference.
*   **FastAPI Backend:**
    *   Provides the entire API infrastructure: offers high performance (asynchronous), automatic interactive documentation (`Swagger UI`, `ReDoc`), robust data handling with Pydantic, and ease of development.
    *   The code defines a `FastAPI` application instance (`app`). Endpoints are defined using FastAPI decorators (`@app.post`, `@app.get`). Pydantic models are used for request/response shaping and validation. The `lifespan` context manager manages the startup/shutdown lifecycle for the `metaRAG` instance. `HTTPException` is used for standard API error reporting (e.g., 400 Bad Request, 422 Unprocessable Entity, 500 Internal Server Error, 503 Service Unavailable).
*   **Configurable via Environment Variables:**
    *   Key settings like the ChromaDB path, Ollama host, and ChromaDB collection name can be easily adjusted without changing the code. This makes deployment and environment management flexible (e.g., using different database locations for development, staging, and production).
    *   The code uses `os.getenv()` to read `CHROMA_DB_PATH`, `OLLAMA_API_HOST`, and `RAG_COLLECTION_NAME` from the environment, falling back to default values if not set. Language-specific models are configured directly in the `DEFAULT_LANGUAGE_MODELS` dictionary within the code, allowing for clear definition of supported languages and their associated models.
*   **Health Check Endpoint:**
    *   A simple `GET /health/` endpoint to quickly check if the API and its critical dependencies (ChromaDB and Ollama) are operational. This is essential for deployment monitoring, load balancer health checks, and rapid diagnosis of service availability issues.
    *   The endpoint function attempts basic interactions with the `ollama_client` (listing models) and the `chromadb` `collection` (getting count). If either interaction fails, it raises an `HTTPException` with a 503 status code, indicating service unavailability and including the error detail.

## Prerequisites

Before running metaRAG server, you need to have installed Ollama and the models:

1.  **Ollama:** Download and install Ollama from [https://ollama.ai/download](https://ollama.ai/download).
2.  **Ollama Models:** Pull the language models required by the application. The default configuration uses:
    *   `nomic-embed-text:137m-v1.5-fp16` (for English embeddings)
    *   `granite-embedding:278m` (for Italian embeddings)
    *   `llama3:8b` (for English LLM)
    *   `mistral-nemo:12b` (for Italian LLM)

    You can pull them using the Ollama command-line interface:
    ```bash
    ollama pull nomic-embed-text:137m-v1.5-fp16
    ollama pull granite-embedding:278m
    ollama pull llama3:8b
    ollama pull mistral-nemo:12b
    # Pull any other models you configure in the code
    ```
    Ensure your Ollama service is running (typically runs in the background after installation or with `ollama serve` command).

## Installation

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/SigfridoCorradi/metaRAG
    cd metaRAG
    ```

2.  **Create a virtual environment (recommended):**
    ```bash
    python -m venv .venv
    ```

3.  **Activate the virtual environment:**
    *   On macOS and Linux:
        ```bash
        source .venv/bin/activate
        ```
    *   On Windows:
        ```bash
        .venv\Scripts\activate
        ```

4.  **Install the dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

## Configuration

The server can be configured using environment variables:

*   `CHROMA_DB_PATH`: Path to the directory where ChromaDB will store its data. Defaults to `./metaRAG_db`.
*   `OLLAMA_API_HOST`: The host and port where the Ollama service is running (e.g., `http://localhost:11434`). Defaults to `None`, which makes the `ollama.Client` use its default connection (usually `http://localhost:11434`). Set this if your Ollama instance is on a different machine or port.
*   `RAG_COLLECTION_NAME`: The name of the collection used in ChromaDB. Defaults to `metaRAG_collection`.
  
The `DEFAULT_LANGUAGE_MODELS` dictionary in the code defines which embedding and LLM models to use for each supported language. You can modify this dictionary to add/remove languages or change models. Ensure the models specified here are pulled and available in Ollama.

```python
# --- Default Model Configuration ---
DEFAULT_LANGUAGE_MODELS = {
    "en": {
        "embedding": "nomic-embed-text:137m-v1.5-fp16",
        "llm": "llama3:8b"
    },
    "it": {
        "embedding": "granite-embedding:278m",
        "llm": "mistral-nemo:12b"
    }
    # Add more languages here if needed
}
```

## Running the Server

Make sure your virtual environment is activated and dependencies are installed.

The recommended way to run the FastAPI application using `uvicorn` is from the command line:

```bash
# Set environment variables if needed (example for Linux/macOS)
# export CHROMA_DB_PATH="./metaRAG_db"
# export OLLAMA_API_HOST="http://localhost:11434"

# Run the server
uvicorn metaRAG:app --reload --host 0.0.0.0 --port 8000
```

*   `metaRAG`: The name of the metaRAG Python file (without the `.py` extension).
*   `app`: The name of the FastAPI instance variable within your file (`app = FastAPI(...)`).
*   `--reload`: (Development only) Restarts the server whenever code changes are detected.
*   `--host 0.0.0.0`: Makes the server accessible externally (if firewalls allow). Use `127.0.0.1` or `localhost` to restrict access to the local machine.
*   `--port 8000`: The port the server will listen on.

The code also includes a section `__main__` that allows running the server programmatically, primarily for simple testing or debugging:

```bash
python metaRAG.py
```
*(Note: The `--reload` flag does not work when running this way programmatically)*

The server should start and print messages to the console, including the initialization status of ChromaDB and Ollama. If metaRAG initialization fails (e.g., cannot connect to Ollama or missing models), the API will return `503 Service Unavailable` errors.

## API Endpoints

The server exposes the following endpoints:

| Path             | Method | Description                                                                                                | Tags    |
| :--------------- | :----- | :--------------------------------------------------------------------------------------------------------- | :------ |
| `/health/`       | `GET`  | Checks the status of the API server and its dependencies (ChromaDB, Ollama).                               | Utility |
| `/upload/text/`  | `POST` | Uploads plain text content and associated metadata for ingestion into the RAG system.                      | Upload  |
| `/upload/file/`  | `POST` | Uploads a document file (PDF or TXT) along with metadata for ingestion. File content is extracted and splitted. | Upload  |
| `/ask/`          | `POST` | Submits a query and metadata filters to retrieve relevant context and generate an answer using an LLM.     | Query   |

## API Documentation

Once the server is running, you can access the automatic API documentation provided by FastAPI:

*   **Swagger UI:** Open your browser and navigate to `http://localhost:8000/docs`
*   **ReDoc:** Open your browser and navigate to `http://localhost:8000/redoc`

These interactive documentation pages allow you to explore the endpoints, their parameters, expected request bodies, and response structures.

## Usage Examples (using `curl`)

Make sure the server is running before trying these examples. Replace `localhost:8000` if your server is running on a different host or port.

### 1. Health Check

```bash
curl -X GET "http://localhost:8000/health/"
```

Example Response:
```json
{
  "status": "success",
  "message": "metaRAG API is healthy.",
  "data": null
}
```
(Or a 503 error if services are down)

### 2. Upload Text

This uploads a piece of text directly. Note the `Content-Type: application/json` header.

```bash
curl -X POST "http://localhost:8000/upload/text/" \
-H "Content-Type: application/json" \
-d '{
      "text_content": "This is a test note in English for test_user_id. It is about cloud computing.",
      "document_metadata": {
        "user_id": "test_user_id",
        "language": "en",
        "topic": "cloud_computing"
      },
      "source_identifier": "test_user_id_cloud_note_001"
    }'
```

Example Response:
```json
{
  "status": "success",
  "message": "Text content from source 'test_user_id_cloud_note_001' uploaded and processed successfully.",
  "data": null
}
```

### 3. Upload File

This uploads a file (e.g., `my_doc.txt` or `my_report.pdf`) along with its metadata. The metadata is sent as a JSON string in a form field named `metadata_json`.

First, create a dummy text file (e.g., `my_doc.txt`) with some content:

```
This is content from a file. It is about project Alpha for test_user_id_2.
The project is critical.
```

Then run the `curl` command. Ensure `my_doc.txt` is in the directory where you run the command, or provide the full path.

```bash
curl -X POST "http://localhost:8000/upload/file/" \
-F "document_file=@my_doc.txt" \
-F "metadata_json='{\"user_id\": \"test_user_id_2\", \"language\": \"en\", \"project\": \"Alpha\", \"status\": \"critical\"}'"
```

Example Response:
```json
{
  "status": "success",
  "message": "File 'my_doc.txt' uploaded and processed successfully.",
  "data": null
}
```

### 4. Ask a Question (for test_user_id's text)

Query the RAG system based on the uploaded text content. The `query_metadata_filter` is crucial and **must** contain `user_id` and `language`, and any other relevant metadata to narrow down the search space.

```bash
curl -X POST "http://localhost:8000/ask/" \
-H "Content-Type: application/json" \
-d '{
      "query": "What is my note about cloud computing?",
      "query_metadata_filter": {
        "user_id": "test_user_id",
        "language": "en",
        "topic": "cloud_computing"
      },
      "n_results_for_context": 2
    }'
```
*(Ensure the `language` in the filter matches the actual language of the content you are querying against, and is a language supported by the server's configuration.)*

Example Response:
```json
{
  "status": "success",
  "message": "Query processed successfully.",
  "data": {
    "answer": "Based on your documents for user 'test_user_id' regarding cloud computing, your note mentions \"This is a test note in English for test_user_id. It is about cloud computing.\"",
    "retrieved_context_items": [
      {
        "text": "This is a test note in English for test_user_id. It is about cloud computing.",
        "metadata": {
          "user_id": "test_user_id",
          "language": "en",
          "topic": "cloud_computing",
          "source_filename": "test_user_id_cloud_note_001"
        },
        "distance": 0.345123
      }
      // ... other context items if retrieved
    ]
  }
}
```

### 5. Ask a Question (for test_user_id_2's file)

Query based on the content uploaded via file. Again, use the metadata filter to scope the search.

```bash
curl -X POST "http://localhost:8000/ask/" \
-H "Content-Type: application/json" \
-d '{
      "query": "What is project Alpha about?",
      "query_metadata_filter": {
        "user_id": "test_user_id_2",
        "language": "en",
        "project": "Alpha"
      },
      "n_results_for_context": 3
    }'
```
*(Ensure the `language` in the filter matches the actual language of the content.)*

Example Response (Answer will vary based on context retrieval and LLM):
```json
{
  "status": "success",
  "message": "Query processed successfully.",
  "data": {
    "answer": "Based on the provided documents matching your filter, project Alpha is described as critical.",
    "retrieved_context_items": [
      {
        "text": "The project is critical.",
        "metadata": {
          "user_id": "test_user_id_2",
          "language": "en",
          "project": "Alpha",
          "status": "critical",
          "source_filename": "my_doc.txt"
        },
        "distance": 0.156789
      },
      // ... other context items if retrieved
    ]
  }
}
```

## Project Structure

```
.
├── metaRAG.py          # The main FastAPI application and metaRAG logic
├── requirements.txt    # Python dependencies
└── metaRAG_db/         # Default directory for ChromaDB persistence (created automatically)
```
