# metaRAG

![FastAPI](https://img.shields.io/badge/FastAPI-009688?style=for-the-badge&logo=fastapi&logoColor=white)
![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)
![ChromaDB](https://img.shields.io/badge/ChromaDB-000000?style=for-the-badge&logo=chroma&logoColor=white)
![Ollama](https://img.shields.io/badge/Ollama-17022A?style=for-the-badge&logo=ollama&logoColor=white)
![Pydantic](https://img.shields.io/badge/Pydantic-E92063?style=for-the-badge&logo=pydantic&logoColor=white)

## Introduction

This project implements a Retrieval Augmented Generation (RAG) system exposed as a RESTful API built with FastAPI. The core purpose is to empower users to upload their text and documents (PDF/TXT) and, critically, **enrich this data with flexible, custom metadata**. This enriched data is then stored in a searchable format, allowing users to ask questions and receive highly relevant, contextually grounded answers generated by a Large Language Model (LLM).

The **metaRAG** implementation make metadata core in the process: by integrating vector databases (ChromaDB) with LLMs (via Ollama), the system overcomes knowledge cutoff dates and hallucinations inherent in base LLMs, but the ability to **define and utilize rich metadata**, associated with each piece of uploaded content, is the core value.

Beyond mandatory fields like `user_id` (essential for data isolation) and `language` (for language-specific model selection), the system allows adding *any* custom key-value pairs to describe your data: be it `topic`, `project`, `department`, `date`, `source_type`, `internal_code` or any other attribute relevant to your use case. This custom metadata isn't merely descriptive: it's actively used for filtering during the retrieval phase.

This use of metadata enables **precise contextual filterinimport chromadb
import ollama
import os
from pathlib import Path
from typing import List, Dict, Optional, Any, Tuple, Union
import time
import shutil
import tempfile
import json
from pypdf import PdfReader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from fastapi import FastAPI, UploadFile, File, Form, HTTPException
from pydantic import BaseModel, Field, model_validator
from contextlib import asynccontextmanager

#Default Model Configuration
DEFAULT_LANGUAGE_MODELS = {
    "en": {
        "embedding": "nomic-embed-text:137m-v1.5-fp16",
        "llm": "llama3:8b"
    },
    "it": {
        "embedding": "granite-embedding:278m",
        "llm": "mistral-nemo:12b"
    }
}

#FastAPI Setup
CHROMA_DB_PATH = os.getenv("CHROMA_DB_PATH", "./metaRAG_db")
OLLAMA_API_HOST = os.getenv("OLLAMA_API_HOST", None)
RAG_COLLECTION_NAME = os.getenv("RAG_COLLECTION_NAME", "metaRAG_collection")

#Pydantic models
class PydanticModel_BaseDocument_Metadata(BaseModel):
    user_id: str = Field(..., description="The ID of the user uploading or querying the document.")
    language: str = Field(..., description="The language code (e.g., 'en', 'it') of the document content.")
    model_config = {
        "extra": "allow"
    }

class PydanticModel_TextUpload_Request(BaseModel):
    text_content: str = Field(..., description="The plain text content to upload.")
    document_metadata: mPydanticModel_BaseDocument_Metadata = Field(..., 
                                                    description="Metadata associated with the text, must include user_id and language.", 
                                                    examples=[{
                                                        "user_id": "test_user_id",
                                                        "language": "it",
                                                        "topic": "test topic value"
                                                        }]
                                                    )
    source_identifier: str = Field("direct_text_upload", description="A string to identify the source of this text, used as 'source_filename' in metadata.")

class PydanticModel_Ask_Request(BaseModel):
    query: str = Field(..., description="The question to ask the RAG system.")
    query_metadata_filter: PydanticModel_BaseDocument_Metadata = Field(..., 
                                                        description="Metadata associated with the text, must include user_id and language.",
                                                        examples=[{
                                                            "user_id": "test_user_id",
                                                            "language": "it",
                                                            "topic": "test topic value"
                                                            }]
                                                        )
    n_results_for_context: int = Field(3, description="Number of context chunks to retrieve.", gt=0)

g**. When the user asks a query through the API, it provides not only the query but also a metadata filter: the system then performs a vector search *only* within the subset of documents (or document chunks) that precisely match *all* criteria in your filter. This guarantees that:

1.  **Data Segregation**: User data remains separate and inaccessible to queries from other users unless explicitly allowed by the filter.
2.  **Targeted Retrieval**: Queries are scoped to the exact set of relevant documents defined.

This **metaRAG** project is built on:

1.  **FastAPI**: It provides the endpoints (`/upload/text`, `/upload/file`, `/ask`, `/health`), handle HTTP requests, and manage the response cycle. Its asynchronous capabilities (`async def`) allow it to handle multiple requests efficiently. FastAPI also automatically generates interactive API documentation (`Swagger UI` and `ReDoc`) based on the code structure and Pydantic models.
2.  **Pydantic**: Integrated seamlessly with FastAPI, Pydantic is used for defining clear data models (`PydanticModel_BaseDocument_Metadata`, `PydanticModel_TextUpload_Request`, `PydanticModel_Ask_Request`, `PydanticModel_Api_Response`, `PydanticModel_AskResponse_Data`, `PydanticModel_ContextItem`) for request and response bodies. This ensures automatic data validation, serialization, and helps FastAPI generate accurate OpenAPI documentation. It enforces that required fields, like `user_id` and `language` in metadata, are present and correctly typed.
3.  **ChromaDB**: This acts as the persistent vector store: it's responsible for storing the numerical representations (embeddings) of document chunks, along with their original text content and associated metadata. During a query, ChromaDB efficiently searches for embeddings similar to the query embedding and, crucially, applies the specified metadata filters (`user_metadata_filter`) to retrieve only relevant chunks from the user's permitted data subset.
4.  **Ollama:** This serves as the engine for running various Language Models. It provides APIs to generate document and query embeddings using dedicated embedding models (like `nomic-embed-text` or `granite-embedding` in the default example) and to generate text answers using larger LLMs (like `llama3` or `mistral-nemo`). The application interacts with Ollama via its Python client library. The configuration allows specifying different embedding and LLM models based on the document/query language (by the *DEFAULT_LANGUAGE_MODELS -> language -> embedding* value).
5.  **Langchain Text Splitters**: A library used to break down large pieces of text (from documents or direct text input) into smaller, fixed-size `chunks`. This is essential because embedding models, and in general RAG systems, perform better with smaller, self-contained pieces of information. The `RecursiveCharacterTextSplitter` is used here to split text while maintaining context where possible.
6.  **pypdf**: A library specifically used to extract text content from uploaded PDF files.
7.  **uvicorn**: ASGI (Asynchronous Server Gateway Interface) server to run the FastAPI application.

**How `metaRAG` orchestrates the activities of the RAG:**

*   **Initialization**: On server startup, the FastAPI `lifespan` context manager initializes a single instance of the `metaRAG` class: this instance connects to ChromaDB (creating or loading the collection) and verifies connectivity and model availability with Ollama.
*   **Ingestion Workflow (`/upload/text`, `/upload/file`)**:
    *   FastAPI receives the request and uses Pydantic to validate the input (text/file content and metadata).
    *   The request handler passes the validated data to the `metaRAG` instance (`upload_text` or `upload_document` methods).
    *   Inside `metaRAG`:
        *   File uploads trigger text extraction using `pypdf`.
        *   Text content (either raw text or extracted from a file) is passed to `langchain_text_splitters` to create chunks.
        *   For each chunk, the `ollama` client is called to generate an embedding using the appropriate language-specific model (determined by the metadata's `language`).
        *   The chunk's text, its embedding, and the document's metadata (including the user-provided fields and system-added `source_filename`) are added to the `ChromaDB` collection using the `ChromaDB` client. Metadata values that are complex Python types (like dicts or lists) are automatically serialized to JSON strings before storage.
*   **Query Workflow (`/ask`)**:
    *   FastAPI receives the query and uses Pydantic (`PydanticModel_Ask_Request`) to validate the input, including the query string and the mandatory `query_metadata_filter`.
    *   The request handler passes the validated data to the `metaRAG.ask` method.
    *   Inside `metaRAG.ask`:
        *   The `ollama` client generates an embedding for the user's query using the language-specific embedding model matching the filter's `language`.
        *   The `chromadb` collection is queried using the query embedding. The `query_metadata_filter` (which *must* include `user_id` and `language`, and can include other custom fields) is passed to ChromaDB's `where` clause to ensure only relevant documents are searched.
        *   ChromaDB returns the most similar document chunks matching the filter.
        *   The text content of the retrieved chunks is compiled into a single context string.
        *   The `ollama` client is called again, this time with the language-specific LLM, providing the compiled context and the original query. The LLM then generates a response based *only* the context provided.
        *   The generated answer and the details of the retrieved context items (including their deserialized metadata) are structured into the `PydanticModel_AskResponse_Data` Pydantic model, wrapped in an `PydanticModel_Ask_Api_Response`, and returned by the FastAPI endpoint.

## Features

This RAG implementation has the following key capabilities:

*   **Document and Text Ingestion**:
    *   Provides FastAPI API endpoints (`/upload/text/` and `/upload/file/`) to get data into the RAG system: plain text or upload PDF and TX for populating the knowledge base with the documents you want to query.
    *   For files: the code temporarily saves the file and uses `pypdf` (for PDFs) or standard file reading (for TXT) to extract the raw text content. This text content is then passed down the ingestion pipeline. Pydantic models (`PydanticModel_TextUpload_Request`, `PydanticModel_BaseDocument_Metadata`) ensure the incoming data format is correct.
*   **Automatic Chunking**:
    *   Automatically breaks down long text content (from uploads) into smaller, more manageable segments called "chunks": LLM context windows are limited, and smaller chunks allow the vector database to find more precise relevant pieces of information during retrieval. Overlap between chunks helps ensure that context isn't lost at the boundaries of the split.
    *   The `metaRAG` class uses `langchain_text_splitters.RecursiveCharacterTextSplitter` configured with a specific `chunk_size` and `chunk_overlap`. After text extraction (for files) or receiving the raw text, the `split_text` method is called to generate the list of chunks.
*   **Rich Metadata Handling and Filtering**:
    *   Allows associating custom key-value metadata (e.g., `user_id` [mandatory], `language` [mandatory], `topic`, `project`, `date`, etc.) with each document during upload. This metadata is stored alongside the document chunks. Query operations can include metadata filters to retrieve context only from relevant documents. This is essential for multi-tenancy (isolating one user's data from another's), categorizing documents, and refining query results. Mandatory `user_id` and `language` fields ensure data segregation and enable language-specific model selection.
    *   The `PydanticModel_BaseDocument_Metadata` Pydantic model allows for `extra="allow"`, meaning you can include any custom fields beyond the required `user_id` and `language`. During ingestion (`_process_and_store_chunks`), this metadata is stored with each chunk in ChromaDB. During querying (`_retrieve_context`), the `query_metadata_filter` (also based on `PydanticModel_BaseDocument_Metadata`) is translated into ChromaDB's `where` filter condition, ensuring searches are limited to chunks matching *all* specified criteria. The `_serialize_metadata_values` method handles converting complex Python objects in metadata to ChromaDB-compatible types (JSON strings).
*   **Vector Embedding Generation**:
    *   Converts the text chunks (during ingestion) and the user's query (during retrieval) into high-dimensional numerical vectors (embeddings): embeddings capture the semantic meaning of the text. By converting text into vectors, ChromaDB can use mathematical operations (calculating cosine similarity) to find text segments with similar meanings, regardless of exact keyword matches.
      * What is *cosine similarity* in this context?
         1.  **Vectors represent *Meaning***:
             *   The main idea is that both the text chunks and the user's query are transformed into "high-dimensional numerical vectors" (embeddings). This trasformation is made by the model (operated by Ollama) defined into *DEFAULT_LANGUAGE_MODELS -> language -> embedding* value.
             *   These embeddings are designed so that texts with similar *meanings* will have vectors that are "close" to each other in this high-dimensional space. "Close" here doesn't just mean physically near if you could plot them, but more about their *orientation*. *Cosine similarity* measures the cosine of the angle between these two vectors, the orientation: it's a way to determine how similar the *direction* of two vectors is.
         
         2.  **Interpreting the cosine similarity score**:
             *   **1**: The angle between the vectors is 0 degrees. They point in the exact same direction. This means the texts they represent are considered very semantically similar.
             *   **0**: The angle is 90 degrees. The vectors are orthogonal (perpendicular). This means the texts are considered to have no semantic similarity (or are unrelated).
             *   **-1**: The angle is 180 degrees. The vectors point in opposite directions. This means the texts are considered to have opposite meanings (though this is less common in typical semantic search and more relevant in tasks like sentiment analysis if embeddings are trained for it).
             *   **between 0 and 1**: (the most common) indicate varying degrees of similarity, the closer to 1 is the more similar.
         
         3.  **Calculating cosine similarity for the user query (text)**:
             *   The query is first converted into its own vector embedding (by the model - operated by Ollama - defined into *DEFAULT_LANGUAGE_MODELS -> language -> embedding* value).
             *   Next, ChromaDB takes this query vector and **compares its direction to the direction of every text chunk vector** it has stored from the ingestion phase by calculating the cosine of the angle between the query vector and each chunk vector (while it might not literally compare against every single vector in a massive dataset due to the use of efficient ANN -Approximate Nearest Neighbor- search algorithms).
             *   The text chunks whose vectors have the highest cosine similarity scores (closest to 1) with the query vector are the most semantically relevant to the user's query.
         
         5.  **Why is it useful for metaRAG and a RAG system in general?**
             *   Example: if the user queries "best ways to travel quickly in a city" and a document chunk talks about "efficient urban transportation methods like subways and electric scooters," traditional keyword search might miss the strong connection.
             *   However, because the embeddings capture *semantic meaning*, the vector for "best ways to travel quickly in a city" will likely point in a very similar direction to the vector for "efficient urban transportation methods." Cosine similarity will pick up on this high directional similarity, even though the exact words are different.
        
    *   The `metaRAG` class interacts with the `ollama.Client` and the `_get_ollama_embedding` method calls the Ollama service, with the appropriate *embedding model* based on the language associated with the text/query (determined by the `language` metadata and the `_get_models_for_language` helper) and the method return the embedding of the received text.
*   **Persistent vector storage**:
    *   Stores the generated vector embeddings, the original text chunks, and their associated metadata in ChromaDB designed for vector search, building the knowledge base.
    *   This project uses `chromadb.PersistentClient` to implement it. The `metaRAG` instance connects to a persistent collection (defaulting to `./metaRAG_db/metaRAG_collection`) and the `_process_and_store_chunks` method uses the `collection.add()` method to store the chunks, embeddings, and metadata.
*   **Metadata contextual retrieval**:
    *   Given a user query and metadata filters, metaRAG (via ChromaDB) searches inside the vector database to find the most semantically relevant document chunks that also match the specified metadata. Combining vector similarity (done by ChromaDB) with precise metadata filtering ensures both semantic relevance *and* adherence to constraints like user access.
    *   The `_retrieve_context` method takes the query embedding and the `query_metadata_filter_dict`: it calls `self.collection.query()`, passing the query embedding, the desired number of results (`n_results`), and constructing ChromaDB's `where` filter from the provided metadata dictionary. The returned `documents`, `metadatas`, and `distances` are then structured into `PydanticModel_ContextItem` Pydantic models.
*   **Use of the Language Model (LLM)**:
    *   The defined LLM acts as the *reasoning* and *text generation* minimizing the risk of hallucination thanks to text chunks from ChromaDB.
    *   The `ask` method takes the retrieved context items (from ChromaDB) and the user query. It selects the appropriate *LLM model* using `_get_models_for_language` based on the filter's language (defined into *DEFAULT_LANGUAGE_MODELS -> language -> embedding* value). The LLM's prompt is constructed using a `prompt_template` that explicitly instructs the LLM to answer based *only* on the provided context: this prompt is sent to the Ollama service via `ollama_client.chat()`, and the LLM's response is extracted as the final answer.
*   **Multi-language Support**:
    *   The system can handle and process documents and queries in different languages for different users (defined into *DEFAULT_LANGUAGE_MODELS*): the use of language-specific embedding models generally improves retrieval accuracy, and using LLMs suitable for the target language enhances answer quality.
    *   The `DEFAULT_LANGUAGE_MODELS` dictionary maps language codes (e.g., "en", "it") to specific Ollama embedding and LLM models. The `language` field in the metadata is mandatory and is used by the `_get_models_for_language` helper to dynamically select the correct models for both embedding generation (ingestion and query) and LLM inference.
*   **FastAPI Backend**:
    *   Provides the entire API infrastructure and offers automatic interactive documentation (`Swagger UI`, `ReDoc`) with robust data handling with Pydantic.
    *   The code defines a `FastAPI` application instance (`app`) and endpoints are defined using FastAPI decorators (`@app.post`, `@app.get`). Pydantic models are used for request/response definition and validation. The `lifespan` context manager manages the startup/shutdown lifecycle for the `metaRAG` instance. `HTTPException` is used for standard API error reporting (e.g., 400 Bad Request, 422 Unprocessable Entity, 500 Internal Server Error, 503 Service Unavailable).
*   **Configurable via Environment Variables**:
    *   Key settings like the ChromaDB path, Ollama host, and ChromaDB collection name (CHROMA_DB_PATH, OLLAMA_API_HOST, RAG_COLLECTION_NAME) can be easily adjusted without changing the code.
    *   The code uses `os.getenv()` to read `CHROMA_DB_PATH`, `OLLAMA_API_HOST`, and `RAG_COLLECTION_NAME` from the environment, falling back to default values if not set.
*   **Health Check Endpoint**:
    *   A simple API `GET /health/` endpoint is useful for quickly check if the API and its critical dependencies (ChromaDB and Ollama) are operational. This is essential for deployment monitoring.
    *   The endpoint function attempts basic interactions with the `ollama_client` (listing models) and the `chromadb` `collection` (getting count): if either interaction fails, it raises an `HTTPException` with a 503 status code, indicating service unavailability and including the error detail.

## Prerequisites

Before running `metaRAG` server, you need to have installed Ollama and pulled the required models:

1.  **Ollama**: Download and install Ollama from [https://ollama.ai/download](https://ollama.ai/download).
2.  **Ollama Models**: Pull the language models required by the application. The default configuration uses:
    *   `nomic-embed-text:137m-v1.5-fp16` (for English embeddings)
    *   `granite-embedding:278m` (for Italian embeddings)
    *   `llama3:8b` (for English LLM)
    *   `mistral-nemo:12b` (for Italian LLM)

    You can pull them using the Ollama command-line interface:
    ```bash
    ollama pull nomic-embed-text:137m-v1.5-fp16
    ollama pull granite-embedding:278m
    ollama pull llama3:8b
    ollama pull mistral-nemo:12b
    # Pull any other models you configure in the code
    ```
    Ensure your Ollama service is running (typically runs in the background after installation or with `ollama serve` command).

## Installation

1.  **Clone the repository**:
    ```bash
    git clone https://github.com/SigfridoCorradi/metaRAG
    cd metaRAG
    ```

2.  **Create a virtual environment (recommended)**:
    ```bash
    python -m venv .venv
    ```

3.  **Activate the virtual environment**:
    *   On macOS and Linux:
        ```bash
        source .venv/bin/activate
        ```
    *   On Windows:
        ```bash
        .venv\Scripts\activate
        ```

4.  **Install the dependencies**:
    ```bash
    pip install -r requirements.txt
    ```

## Configuration

The server can be configured using environment variables:

*   `CHROMA_DB_PATH`: Path to the directory where ChromaDB will store its data. Defaults to `./metaRAG_db`.
*   `OLLAMA_API_HOST`: The host and port where the Ollama service is running (e.g., `http://localhost:11434`). Defaults to `None`, which makes the `ollama.Client` use its default connection (usually `http://localhost:11434`). Set this if your Ollama instance is on a different machine or port.
*   `RAG_COLLECTION_NAME`: The name of the collection used in ChromaDB. Defaults to `metaRAG_collection`.
  
The `DEFAULT_LANGUAGE_MODELS` dictionary in the code defines which embedding and LLM models to use for each supported language. You can modify this dictionary to add/remove languages or change models. Ensure the models specified here are pulled and available in Ollama.

```python
# --- Default Model Configuration ---
DEFAULT_LANGUAGE_MODELS = {
    "en": {
        "embedding": "nomic-embed-text:137m-v1.5-fp16",
        "llm": "llama3:8b"
    },
    "it": {
        "embedding": "granite-embedding:278m",
        "llm": "mistral-nemo:12b"
    }
    # Add more languages here if needed
}
```

## Running the Server

Make sure your virtual environment is activated and dependencies are installed.

The recommended way to run the FastAPI application using `uvicorn` is from the command line:

```bash
# Set environment variables if needed (example for Linux)
# export CHROMA_DB_PATH="./metaRAG_db"
# export OLLAMA_API_HOST="http://localhost:11434"

# Run the server
uvicorn metaRAG:app --reload --host 0.0.0.0 --port 8000
```

*   `metaRAG`: The name of the metaRAG Python file (without the `.py` extension).
*   `app`: The name of the FastAPI instance variable within your file (`app = FastAPI(...)`).
*   `--reload`: (Development only) Restarts the server whenever code changes are detected.
*   `--host 0.0.0.0`: Makes the server accessible externally (if firewalls allow). Use `127.0.0.1` or `localhost` to restrict access to the local machine.
*   `--port 8000`: The port the server will listen on.

The code also includes a section `__main__` that allows running the server programmatically, primarily for simple testing or debugging:

```bash
python metaRAG.py
```
*(Note: The `--reload` flag does not work when running this way programmatically)*

The server should start and print messages to the console, including the initialization status of ChromaDB and Ollama. If metaRAG initialization fails (e.g., cannot connect to Ollama or missing models), the API will return `503 Service Unavailable` errors.

## API Endpoints

The server exposes the following endpoints:

| Path             | Method | Description                                                                                                | Tags    |
| :--------------- | :----- | :--------------------------------------------------------------------------------------------------------- | :------ |
| `/health/`       | `GET`  | Checks the status of the API server and its dependencies (ChromaDB, Ollama).                               | Utility |
| `/upload/text/`  | `POST` | Uploads plain text content and associated metadata for ingestion into the RAG system.                      | Upload  |
| `/upload/file/`  | `POST` | Uploads a document file (PDF or TXT) along with metadata for ingestion. File content is extracted and splitted. | Upload  |
| `/ask/`          | `POST` | Submits a query and metadata filters to retrieve relevant context and generate an answer using an LLM.     | Query   |

## API Documentation

Once the server is running, you can access the automatic API documentation provided by FastAPI:

*   **Swagger UI**: Open your browser and navigate to `http://localhost:8000/docs`
*   **ReDoc**: Open your browser and navigate to `http://localhost:8000/redoc`

These interactive documentation pages allow you to explore the endpoints, their parameters, expected request bodies, and response structures.

## Usage Examples (using `curl`)

Make sure the server is running before trying these examples. Replace `localhost:8000` if your server is running on a different host or port.

### 1. Health Check

```bash
curl -X GET "http://localhost:8000/health/"
```

Example Response:
```json
{
  "status": "success",
  "message": "metaRAG API is healthy.",
  "data": null
}
```
(Or a 503 error if services are down)

### 2. Upload Text

This uploads a piece of text directly. Note the `Content-Type: application/json` header.

```bash
curl -X POST "http://localhost:8000/upload/text/" \
-H "Content-Type: application/json" \
-d '{
      "text_content": "This is a test note in English for test_user_id. It is about cloud computing.",
      "document_metadata": {
        "user_id": "test_user_id",
        "language": "en",
        "topic": "cloud_computing"
      },
      "source_identifier": "test_user_id_cloud_note_001"
    }'
```

Example Response:
```json
{
  "status": "success",
  "message": "Text content from source 'test_user_id_cloud_note_001' uploaded and processed successfully.",
  "data": null
}
```

### 3. Upload File

This uploads a file (e.g., `my_doc.txt` or `my_report.pdf`) along with its metadata. The metadata is sent as a JSON string in a form field named `metadata_json`.

First, create a dummy text file (e.g., `my_doc.txt`) with some content:

```
This is content from a file. It is about project Alpha for test_user_id_2.
The project is critical.
```

Then run the `curl` command. Ensure `my_doc.txt` is in the directory where you run the command, or provide the full path.

```bash
curl -X POST "http://localhost:8000/upload/file/" \
-F "document_file=@my_doc.txt" \
-F "metadata_json='{\"user_id\": \"test_user_id_2\", \"language\": \"en\", \"project\": \"Alpha\", \"status\": \"critical\"}'"
```

Example Response:
```json
{
  "status": "success",
  "message": "File 'my_doc.txt' uploaded and processed successfully.",
  "data": null
}
```

### 4. Ask a Question (for test_user_id's text)

Query the RAG system based on the uploaded text content. The `query_metadata_filter` is crucial and **must** contain `user_id` and `language`, and any other relevant metadata to narrow down the search space.

```bash
curl -X POST "http://localhost:8000/ask/" \
-H "Content-Type: application/json" \
-d '{
      "query": "What is my note about cloud computing?",
      "query_metadata_filter": {
        "user_id": "test_user_id",
        "language": "en",
        "topic": "cloud_computing"
      },
      "n_results_for_context": 2
    }'
```
*(Ensure the `language` in the filter matches the actual language of the content you are querying against, and is a language supported by the server's configuration.)*

Example Response:
```json
{
  "status": "success",
  "message": "Query processed successfully.",
  "data": {
    "answer": "Based on your documents for user 'test_user_id' regarding cloud computing, your note mentions \"This is a test note in English for test_user_id. It is about cloud computing.\"",
    "retrieved_context_items": [
      {
        "text": "This is a test note in English for test_user_id. It is about cloud computing.",
        "metadata": {
          "user_id": "test_user_id",
          "language": "en",
          "topic": "cloud_computing",
          "source_filename": "test_user_id_cloud_note_001"
        },
        "distance": 0.345123
      }
      // ... other context items if retrieved
    ]
  }
}
```

### 5. Ask a Question (for test_user_id_2's file)

Query based on the content uploaded via file. Again, use the metadata filter to scope the search.

```bash
curl -X POST "http://localhost:8000/ask/" \
-H "Content-Type: application/json" \
-d '{
      "query": "What is project Alpha about?",
      "query_metadata_filter": {
        "user_id": "test_user_id_2",
        "language": "en",
        "project": "Alpha"
      },
      "n_results_for_context": 3
    }'
```
*(Ensure the `language` in the filter matches the actual language of the content.)*

Example Response (Answer will vary based on context retrieval and LLM):
```json
{
  "status": "success",
  "message": "Query processed successfully.",
  "data": {
    "answer": "Based on the provided documents matching your filter, project Alpha is described as critical.",
    "retrieved_context_items": [
      {
        "text": "The project is critical.",
        "metadata": {
          "user_id": "test_user_id_2",
          "language": "en",
          "project": "Alpha",
          "status": "critical",
          "source_filename": "my_doc.txt"
        },
        "distance": 0.156789
      },
      // ... other context items if retrieved
    ]
  }
}
```

## Project Structure

```
.
├── metaRAG.py          # The main FastAPI application and metaRAG logic
├── requirements.txt    # Python dependencies
└── metaRAG_db/         # Default directory for ChromaDB persistence (created automatically)
```
